---
layout: page
title: Reinforcement Learning from Human Feedback
description: How to scale RL to combinatorially sized language action spaces and messy human preference rewards?
img: assets/img/rlhf.png
importance: 1
category: Human Preference Alignment
related_publications: wu2025context, xie2025survey, Ramamurthy2022IsRL, hausknecht2020interactive, martin2017improvisational, ammanabrolu2021motivate, wu2023finegrained, lu2023inference
---

The ultimate aim of language technology is to interact with humans.
However, most such systems are trained without direct signals of human preference, with supervised target strings serving as (a sometimes crude) proxy. This work focuses on using reinforcement learning to interact and align to human preferences.

